{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial with Graph Convolutional Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 方針\n",
    "\n",
    "### 教師あり学習\n",
    "\n",
    "まずは教師あり学習により強いポリシーを獲得。\n",
    "\n",
    "### 自己対戦による訓練\n",
    "\n",
    "* Q関数が相手によって変わってしまう。\n",
    "    * On-policyで自己対戦にすればOK。Off-policyにしたい場合は、Q関数が大きく違わない過去エピソードとすべき。\n",
    "        * Rainbow\n",
    "            * PFRLに実装あり。まずはこれ？\n",
    "        * MuZero\n",
    "            * 実装少し大変かも。だが自己対戦による実績あるため性能は出るかも？\n",
    "    * Policyを持つアルゴリズムならOK。相手ごとにQ関数を学習すれば良い。この場合も自己対戦かつOn-policy。\n",
    "        * AC3、PPOなど\n",
    "            * PFRLに実装あり。これもトライ？\n",
    "            \n",
    "### 方策／Q関数モデル\n",
    "\n",
    "* Graph Neural Network\n",
    "    * あたかもそれぞれの選手が行動判断／価値判断しているようなモデルにする。選択されたアクションは、その時Activeな選手の最善手とする。\n",
    "    * 初めは、特徴量は、絶対位置座標で、完全グラフを用いる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions\n",
    "\n",
    "### Default action set\n",
    "\n",
    "The default action set consists of 19 actions:\n",
    "\n",
    "*   Idle actions\n",
    "\n",
    "    *   `action_idle` = 0, a no-op action, stickly actions are not affected (player maintains his directional movement etc.).\n",
    "\n",
    "*   Movement actions\n",
    "\n",
    "    *   `action_left` = 1, run to the left, sticky action.\n",
    "    *   `action_top_left` = 2, run to the top-left, sticky action.\n",
    "    *   `action_top` = 3, run to the top, sticky action.\n",
    "    *   `action_top_right` = 4, run to the top-right, sticky action.\n",
    "    *   `action_right` = 5, run to the right, sticky action.\n",
    "    *   `action_bottom_right` = 6, run to the bottom-right, sticky action.\n",
    "    *   `action_bottom` = 7, run to the bottom, sticky action.\n",
    "    *   `action_bottom_left` = 8, run to the bottom-left, sticky action.\n",
    "\n",
    "*   Passing / Shooting\n",
    "\n",
    "    *   `action_long_pass` = 9, perform a long pass to the player on your team. Player to pass the ball to is auto-determined based on the movement direction.\n",
    "    *   `action_high_pass` = 10, perform a high pass, similar to `action_long_pass`.\n",
    "    *   `action_short_pass` = 11, perform a short pass, similar to `action_long_pass`.\n",
    "    *   `action_shot` = 12, perform a shot, always in the direction of the opponent's goal.\n",
    "\n",
    "*   Other actions\n",
    "\n",
    "    *   `action_sprint` = 13, start sprinting, sticky action. Player moves faster, but has worse ball handling.\n",
    "    *   `action_release_direction` = 14, reset current movement direction.\n",
    "    *   `action_release_sprint` = 15, stop sprinting.\n",
    "    *   `action_sliding` = 16, perform a slide (effective when not having a ball).\n",
    "    *   `action_dribble` = 17, start dribbling (effective when having a ball), sticky action. Player moves slower, but it is harder to take over the ball from him.\n",
    "    *   `action_release_dribble` = 18, stop dribbling.\n",
    "\n",
    "### V2 action set\n",
    "\n",
    "It is an extension of the default action set:\n",
    "\n",
    "*   `action_builtin_ai` = 19, let game's built-in AI generate an action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-output": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Install:\n",
    "# Kaggle environments.\n",
    "#!git clone https://github.com/Kaggle/kaggle-environments.git\n",
    "#!cd kaggle-environments && pip install .\n",
    "\n",
    "# GFootball environment.\n",
    "#!apt-get update -y\n",
    "#!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev\n",
    "\n",
    "# Make sure that the Branch in git clone and in wget call matches !!\n",
    "#!git clone -b v2.3 https://github.com/google-research/football.git\n",
    "#!mkdir -p football/third_party/gfootball_engine/lib\n",
    "\n",
    "#!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.3.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so\n",
    "#!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ install torch_geometric begin -----------------\n",
    "try:\n",
    "    import torch_geometric\n",
    "except:\n",
    "    import subprocess\n",
    "    import torch\n",
    "\n",
    "    nvcc_stdout = str(subprocess.check_output(['nvcc', '-V']))\n",
    "    tmp = nvcc_stdout[nvcc_stdout.rfind('release') + len('release') + 1:]\n",
    "    cuda_version = tmp[:tmp.find(',')]\n",
    "    cuda = {\n",
    "            '9.2': 'cu92',\n",
    "            '10.1': 'cu101',\n",
    "            '10.2': 'cu102',\n",
    "            }\n",
    "\n",
    "    CUDA = cuda[cuda_version]\n",
    "    TORCH = torch.__version__.split('.')\n",
    "    TORCH[-1] = '0'\n",
    "    TORCH = '.'.join(TORCH)\n",
    "\n",
    "    install1 = 'pip install torch-scatter==latest+' + CUDA + ' -f https://pytorch-geometric.com/whl/torch-' + TORCH + '.html'\n",
    "    install2 = 'pip install torch-sparse==latest+' + CUDA + ' -f https://pytorch-geometric.com/whl/torch-' + TORCH + '.html'\n",
    "    install3 = 'pip install torch-cluster==latest+' + CUDA + ' -f https://pytorch-geometric.com/whl/torch-' + TORCH + '.html'\n",
    "    install4 = 'pip install torch-spline-conv==latest+' + CUDA + ' -f https://pytorch-geometric.com/whl/torch-' + TORCH + '.html'\n",
    "    install5 = 'pip install torch-geometric'\n",
    "\n",
    "    subprocess.run(install1.split())\n",
    "    subprocess.run(install2.split())\n",
    "    subprocess.run(install3.split())\n",
    "    subprocess.run(install4.split())\n",
    "    subprocess.run(install5.split())\n",
    "# ------------------ install torch_geometric end -----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import glob \n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "import copy\n",
    "import imageio\n",
    "import pathlib\n",
    "import collections\n",
    "from collections import deque, namedtuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "\n",
    "from gym import spaces\n",
    "from tqdm import tqdm\n",
    "from logging import getLogger, StreamHandler, FileHandler, DEBUG, INFO\n",
    "from typing import Union, Callable, List, Tuple, Iterable, Any, Dict\n",
    "from dataclasses import dataclass\n",
    "from IPython.display import Image, display\n",
    "sns.set()\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as D\n",
    "\n",
    "# PyTorch geometric\n",
    "from torch_geometric.data import Data, DataLoader, Batch\n",
    "from torch_geometric.nn import RGCNConv\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "\n",
    "from torch_scatter import scatter_max, scatter_sum, scatter_mean\n",
    "\n",
    "# Env\n",
    "import gym\n",
    "import gfootball\n",
    "import gfootball.env as football_env\n",
    "from gfootball.env import observation_preprocessing\n",
    "from gfootball.env.wrappers import Simple115StateWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Check we can use GPU\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# set gpu id\n",
    "if torch.cuda.is_available(): \n",
    "    # NOTE: it is not number of gpu but id which start from 0\n",
    "    gpu = 0\n",
    "else:\n",
    "    # cpu=>-1\n",
    "    gpu = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set logger\n",
    "def logger_config():\n",
    "    logger = getLogger(__name__)\n",
    "    handler = StreamHandler()\n",
    "    handler.setLevel(\"DEBUG\")\n",
    "    logger.setLevel(\"DEBUG\")\n",
    "    logger.addHandler(handler)\n",
    "    logger.propagate = False\n",
    "\n",
    "    filepath = './result.log'\n",
    "    file_handler = FileHandler(filepath)\n",
    "    logger.addHandler(file_handler)\n",
    "    return logger\n",
    "\n",
    "logger = logger_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed random seed\n",
    "# but this is NOT enough to fix the result of rewards.Please tell me the reason.\n",
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "# Set a random seed used in PFRL.\n",
    "seed = 5046\n",
    "seed_everything(seed)\n",
    "\n",
    "# Set different random seeds for train and test envs.\n",
    "train_seed = seed\n",
    "test_seed = 2 ** 31 - 1 - seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = football_env.create_environment(\n",
    "    env_name='11_vs_11_kaggle',  # easy mode\n",
    "    stacked=False,\n",
    "    representation='simple115v2',           # SMM\n",
    "    rewards='scoring, checkpoints',\n",
    "    write_goal_dumps=False,\n",
    "    write_full_episode_dumps=False,\n",
    "    render=False,\n",
    "    write_video=False,\n",
    "    dump_frequency=1,\n",
    "    logdir='./',\n",
    "    extra_players=None,\n",
    "    number_of_left_players_agent_controls=1,\n",
    "    number_of_right_players_agent_controls=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, _, _, info = env.step([1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCN-Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The flag to distinguish left players, right players, and the ball.\n",
    "\n",
    "* left players: 0\n",
    "* right players: 1\n",
    "* ball: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_dict = {\n",
    "    (0., 0.): 0, # left player -> left player\n",
    "    (0., 1.): 1, # left player -> right player\n",
    "    (0., 2.): 2, # left player -> ball\n",
    "    (1., 0.): 3, # right player -> left player\n",
    "    (1., 1.): 4, # right player -> right player\n",
    "    (1., 2.): 5, # right player -> ball\n",
    "    (2., 0.): 6, # ball -> left player\n",
    "    (2., 1.): 7, # ball -> right player\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_from_observation(state, action=None, reverse=False, reverse_y=False):\n",
    "    array = state\n",
    "    \n",
    "    left_coordinations = np.concatenate([array[:22].reshape(11, 2), np.zeros((11, 1))], axis=-1)\n",
    "    left_directions = np.concatenate([array[22:44].reshape(11, 2), np.zeros((11, 1))], axis=-1)\n",
    "    right_coordinations = np.concatenate([array[44:66].reshape(11, 2), np.zeros((11, 1))], axis=-1)\n",
    "    right_directions = np.concatenate([array[66:88].reshape(11, 2), np.zeros((11, 1))], axis=-1)\n",
    "    ball_coordination = array[88:91].reshape([1, 3])\n",
    "    ball_direction = array[91:94].reshape([1, 3])\n",
    "    ball_ownership = array[94:97] # none, left, right\n",
    "    active_player = array[97:108].reshape([11, 1])\n",
    "    game_mode = array[108:]\n",
    "    \n",
    "    if reverse:\n",
    "        left_coordinations[:, 0] *= -1.0\n",
    "        left_directions[:, 0] *= -1.0\n",
    "        right_coordinations[:, 0] *= -1.0\n",
    "        right_directions[:, 0] *= -1.0\n",
    "        ball_coordination[:, 0] *= -1.0\n",
    "        ball_direction[:, 0] *= -1.0\n",
    "        ball_ownership = ball_ownership[[0, 2, 1]]\n",
    "        active_player = active_player\n",
    "        game_mode = game_mode\n",
    "\n",
    "    if reverse_y:\n",
    "        left_coordinations[:, 1] *= -1.0\n",
    "        left_directions[:, 1] *= -1.0\n",
    "        right_coordinations[:, 1] *= -1.0\n",
    "        right_directions[:, 1] *= -1.0\n",
    "        ball_coordination[:, 1] *= -1.0\n",
    "        ball_direction[:, 1] *= -1.0\n",
    "        ball_ownership = ball_ownership\n",
    "        active_player = active_player\n",
    "        game_mode = game_mode\n",
    "\n",
    "    # Node features\n",
    "    left_features = np.concatenate([\n",
    "        0*np.ones((11, 1)),\n",
    "        left_coordinations,\n",
    "        left_directions,\n",
    "        ball_ownership[1]*np.ones((11, 1)),\n",
    "        active_player,\n",
    "    ], axis=-1)\n",
    "    right_features = np.concatenate([\n",
    "        1*np.ones((11, 1)),\n",
    "        right_coordinations,\n",
    "        right_directions,\n",
    "        ball_ownership[2]*np.ones((11, 1)),\n",
    "        np.zeros((11, 1)),\n",
    "    ], axis=-1)\n",
    "    ball_features = np.concatenate([\n",
    "        2*np.ones((1, 1)),\n",
    "        ball_coordination,\n",
    "        ball_direction,\n",
    "        np.zeros((1, 1)),\n",
    "        np.zeros((1, 1)),\n",
    "    ], axis=-1)\n",
    "\n",
    "    features = np.concatenate([left_features, right_features, ball_features], axis=0)\n",
    "\n",
    "    # Edges and relations\n",
    "    X, Y = np.meshgrid(np.arange(len(features)), np.arange(len(features)))\n",
    "    all_combinations = np.vstack([X.flatten(), Y.flatten()]).T\n",
    "    edge_index = np.array(\n",
    "        [combination for combination in all_combinations if not combination[0] == combination[1]]\n",
    "    ).T\n",
    "    types_for_edge_index = features[edge_index][:,:,0]\n",
    "    relations = [relations_dict[tuple(types)] for types in types_for_edge_index.T]\n",
    "\n",
    "    # numpy array to torch tensor\n",
    "    features = torch.tensor(features, dtype=torch.float).contiguous()\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).contiguous()\n",
    "    relations = torch.tensor(relations, dtype=torch.long).contiguous()\n",
    "\n",
    "    if action is None:\n",
    "        graph = Data(x=features, edge_index=edge_index, edge_type=relations)\n",
    "\n",
    "    else:\n",
    "        graph = Data(x=features, edge_index=edge_index, edge_type=relations, y=torch.tensor(action, dtype=torch.long))\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_relations):\n",
    "        super().__init__()\n",
    "        self.conv1 = RGCNConv(num_features - 2, 256, num_relations=num_relations)\n",
    "        #self.conv2 = RGCNConv(128, 256, num_relations=num_relations)\n",
    "        self.fc1 = nn.Linear(256, 256)\n",
    "        self.fc2 = nn.Linear(256, 19)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_type = data.x, data.edge_index, data.edge_type\n",
    "        flag = x[:,0]\n",
    "        is_active = x[:,-1]\n",
    "        x = x[:,1:-1]\n",
    "\n",
    "        x = self.conv1(x, edge_index, edge_type)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        #x = self.conv2(x, edge_index, edge_type)\n",
    "        #x = F.relu(x)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return D.Categorical(logits=x[is_active.bool()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_relations):\n",
    "        super().__init__()\n",
    "        self.conv1 = RGCNConv(num_features - 2, 256, num_relations=num_relations)\n",
    "        #self.conv2 = RGCNConv(128, 256, num_relations=num_relations)\n",
    "        self.fc1 = nn.Linear(256, 256)\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_type = data.x, data.edge_index, data.edge_type\n",
    "        flag = x[:,0]\n",
    "        is_active = x[:,-1]\n",
    "        x = x[:,1:-1]\n",
    "\n",
    "        x = self.conv1(x, edge_index, edge_type)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        #x = self.conv2(x, edge_index, edge_type)\n",
    "        #x = F.relu(x)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        if hasattr(data, 'batch'):\n",
    "            x, argmax = scatter_max(x, data.batch, dim=0)\n",
    "        else:\n",
    "            x = torch.max(x, dim=0).values\n",
    "\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load initial policy (Supervised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy(num_features=9, num_relations=len(relations_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_path = 'policy.pt'\n",
    "policy.load_state_dict(torch.load(policy_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Episodes:\n",
    "    def __init__(\n",
    "        self,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.episodes = {\n",
    "            **kwargs,\n",
    "        }\n",
    "        for key in ('state', 'action', 'next_state', 'reward', 'done'):\n",
    "            if key not in kwargs.keys():\n",
    "                self.episodes[key] = []\n",
    "        for key, value in self.episodes.items():\n",
    "            if len(value) != len(self.episodes['state']):\n",
    "                raise Exception('The length of {} is invalid.'.format(key))\n",
    "        \n",
    "    def append(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            self.episodes[key].append(value)\n",
    "        \n",
    "    def add_new_key(self, key, value):\n",
    "        if len(value) != len(self):\n",
    "            raise Exception('The length of {} is invalid.'.format(key))\n",
    "        elif key in self.episodes.keys():\n",
    "            raise Exception('The key {} is already defined.'.format(key))\n",
    "        self.episodes[key] = value\n",
    "        \n",
    "    def update(self, key, value):\n",
    "        if len(value) != len(self):\n",
    "            raise Exception('The length of {} is invalid.'.format(key))\n",
    "        self.episodes[key] = value\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.episodes['state'])\n",
    "    \n",
    "    @property\n",
    "    def num_episodes(self):\n",
    "        return np.sum(self.episodes['done'])\n",
    "    \n",
    "    @property\n",
    "    def total_rewards(self):\n",
    "        rewards = []\n",
    "        total_reward = 0\n",
    "        for reward, done in zip(self.episodes['reward'], self.episodes['done']):\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                rewards.append(total_reward)\n",
    "                total_reward = 0\n",
    "        return rewards\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'Episodes\\n* number of episodes: {}\\n* number of steps: {}\\n* total rewards: {}\\n* keys: {}'.format(\n",
    "            np.sum(self.episodes['done']),\n",
    "            len(self),\n",
    "            self.total_rewards,\n",
    "            ', '.join(self.episodes.keys()),\n",
    "        )\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        return self.episodes[key]\n",
    "\n",
    "    def sample(self, size, sequential=True):\n",
    "        if sequential:\n",
    "            id_from = random.randint(0, len(self) - size)\n",
    "            id_to = id_from + size\n",
    "            sampled_dict = {\n",
    "                key: value[id_from:id_to]\n",
    "                for key, value\n",
    "                in self.episodes.items()\n",
    "            }\n",
    "            sampled_episode = Episodes(\n",
    "                **sampled_dict\n",
    "            )\n",
    "            return sampled_episode\n",
    "        else:\n",
    "            raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Episodic functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(episodes, gamma, infinite_horizon=True, update=False):\n",
    "    rewards = episodes['reward']\n",
    "    states = episodes['state']\n",
    "    dones = episodes['done']\n",
    "    values = episodes['value']\n",
    "\n",
    "    returns = torch.empty(len(rewards), dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        last_reward = infinite_horizon * float(values[-1])\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            returns[t] = last_reward = rewards[t] + (1 - dones[t]) * gamma * last_reward\n",
    "            if infinite_horizon and dones[t]:\n",
    "                reward_to_go = float(values[t])\n",
    "                returns[t] = reward_to_go\n",
    "                last_reward = reward_to_go\n",
    "    if update:\n",
    "        episodes.update('return', returns)\n",
    "    else:\n",
    "        episodes.add_new_key('return', returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_values(episodes, value_net, device='cpu', update=False):\n",
    "    batch = Batch.from_data_list(episodes.episodes['state'])\n",
    "    \n",
    "    batch.to(device)\n",
    "    value_net.to(device)\n",
    "    \n",
    "    values = value_net(batch).flatten()\n",
    "    if update:\n",
    "        episodes.update('value', values)\n",
    "    else:\n",
    "        episodes.add_new_key('value', values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_advantages(episodes, gamma, lambda_=1, device='cpu', update=False):\n",
    "    \"\"\"Generalized Advantage Estimation\"\"\"\n",
    "    rewards = episodes['reward']\n",
    "    dones = episodes['done']\n",
    "    zero = torch.tensor([0]).to(device)\n",
    "    values = torch.cat((episodes['value'], zero), dim=0)\n",
    "    advantages = torch.empty(len(rewards), dtype=torch.float32).to(device)\n",
    "    last_advantage = 0\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        delta = rewards[t] + (1 - dones[t]) * gamma * values[t + 1] - values[t]\n",
    "        advantages[t] = last_advantage = delta + gamma * lambda_ * last_advantage\n",
    "    if update:\n",
    "        episodes.update('advantage', advantages)\n",
    "    else:\n",
    "        episodes.add_new_key('advantage', advantages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_episodes(episodes_list):\n",
    "    keys = episodes_list[0].episodes.keys()\n",
    "    episodes_dict = {key: [] for key in keys}\n",
    "    for episodes in episodes_list:\n",
    "        for key in keys:\n",
    "            episodes_dict[key] += episodes.episodes[key]\n",
    "    return Episodes(**episodes_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value_loss(episodes, device='cpu'):\n",
    "    return F.mse_loss(episodes['value'].to(device), episodes['return'].detach().to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ppo_losses(\n",
    "    episodes,\n",
    "    policy,\n",
    "    policy_old,\n",
    "    clip=False,\n",
    "    clip_param=0.2,\n",
    "    compute_kl=True,\n",
    "    compute_entropy=True,\n",
    "    device='cpu',\n",
    "):\n",
    "    states = episodes['state']\n",
    "    actions = torch.tensor(episodes['action']).to(device)\n",
    "    advantages = episodes['advantage'].detach()\n",
    "    batch_states = Batch.from_data_list(states)\n",
    "    \n",
    "    #actions.to(device)\n",
    "    batch_states.to(device)\n",
    "    policy.to(device)\n",
    "    policy_old.to(device)\n",
    "    \n",
    "    distribs = policy(batch_states)\n",
    "    log_probs = distribs.log_prob(actions)\n",
    "    with torch.no_grad():\n",
    "        distribs_old = policy_old(batch_states)\n",
    "        log_probs_old = distribs_old.log_prob(actions)\n",
    "    prob_ratios = torch.exp(log_probs - log_probs_old)\n",
    "    \n",
    "    if clip:\n",
    "        clipped_prob_ratios = torch.clamp(prob_ratios, 1.0 - clip_param, 1.0 + clip_param) * advantages\n",
    "        policy_loss = -torch.mean(torch.min(prob_ratios, clipped_prob_ratios))\n",
    "    else:\n",
    "        policy_loss = -torch.mean(prob_ratios * advantages)\n",
    "    \n",
    "    losses = {'policy_loss': policy_loss}\n",
    "    \n",
    "    if compute_kl:\n",
    "        kl = D.kl_divergence(distribs_old, distribs).mean()\n",
    "        losses['kl'] = kl\n",
    "    \n",
    "    if compute_entropy:\n",
    "        entropy = distribs.entropy().mean()\n",
    "        losses['entropy'] = entropy\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REVERSE_ACTION = {i: i for i in range(20)}\n",
    "REVERSE_ACTION[1] = 5\n",
    "REVERSE_ACTION[5] = 1\n",
    "REVERSE_ACTION[2] = 4\n",
    "REVERSE_ACTION[4] = 2\n",
    "REVERSE_ACTION[6] = 8\n",
    "REVERSE_ACTION[8] = 6\n",
    "def reverse_action(action):\n",
    "    return REVERSE_ACTION[action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = football_env.create_environment(\n",
    "    env_name='11_vs_11_kaggle',\n",
    "    stacked=False,\n",
    "    representation='simple115v2',\n",
    "    rewards='scoring, checkpoints',\n",
    "    write_goal_dumps=False,\n",
    "    write_full_episode_dumps=False,\n",
    "    render=False,\n",
    "    write_video=False,\n",
    "    dump_frequency=1,\n",
    "    logdir='./',\n",
    "    extra_players=None,\n",
    "    number_of_left_players_agent_controls=1,\n",
    "    number_of_right_players_agent_controls=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_eval = football_env.create_environment(\n",
    "    env_name='11_vs_11_hard_stochastic',  # hard mode\n",
    "    stacked=False,\n",
    "    representation='simple115v2',\n",
    "    rewards='scoring, checkpoints',\n",
    "    write_goal_dumps=False,\n",
    "    write_full_episode_dumps=False,\n",
    "    render=False,\n",
    "    write_video=False,\n",
    "    dump_frequency=1,\n",
    "    logdir='./',\n",
    "    extra_players=None,\n",
    "    number_of_left_players_agent_controls=1,\n",
    "    number_of_right_players_agent_controls=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_episodes(policy):\n",
    "    \n",
    "    episode_l = Episodes()\n",
    "    episode_r = Episodes()\n",
    "\n",
    "    obs_l, obs_r = env.reset()\n",
    "    next_state_l = create_graph_from_observation(obs_l)\n",
    "    next_state_r = create_graph_from_observation(obs_r)\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        state_l, state_r = next_state_l, next_state_r\n",
    "        states = torch_geometric.data.Batch.from_data_list([\n",
    "            state_l, state_r\n",
    "        ])\n",
    "\n",
    "        action_l, action_r = policy(states).sample().detach().numpy().tolist()\n",
    "        #action_r = reverse_action(action_r_rev)\n",
    "\n",
    "        (obs_l, obs_r), (reward_l, reward_r), done, info = env.step([action_l, action_r])\n",
    "\n",
    "        next_state_l = create_graph_from_observation(obs_l)\n",
    "        next_state_r = create_graph_from_observation(obs_r)\n",
    "\n",
    "        episode_l.append(\n",
    "            state=state_l,\n",
    "            action=action_l,\n",
    "            reward=reward_l,\n",
    "            next_state=next_state_l,\n",
    "            done=done,\n",
    "        )\n",
    "        episode_r.append(\n",
    "            state=state_r,\n",
    "            action=action_r,\n",
    "            reward=reward_r,\n",
    "            next_state=next_state_r,\n",
    "            done=done,\n",
    "        )\n",
    "        \n",
    "    return episode_l, episode_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(obs):\n",
    "    global policy\n",
    "    \n",
    "    # Get observations for the first (and only one) player we control.\n",
    "    #obs = Simple115StateWrapper.convert_observation(obs['players_raw'], True)\n",
    "    graph = create_graph_from_observation(obs)\n",
    "    actions = policy(graph)\n",
    "    action = int(np.argmax(actions.detach().numpy()))\n",
    "    return [action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(policy):\n",
    "    obs = env_eval.reset()\n",
    "    rewards = []\n",
    "    while True:\n",
    "        graph = create_graph_from_observation(obs)\n",
    "        with torch.no_grad():\n",
    "            distrib = policy(graph)\n",
    "        action = [int(distrib.logits.argmax())]\n",
    "        obs, reward, done, info = env_eval.step(action)\n",
    "        rewards.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "    score = np.sum(rewards)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.997\n",
    "target_kld = 0.001\n",
    "entropy_reg = 0.0001\n",
    "lr_value = 1e-4\n",
    "lr_policy = 1e-4\n",
    "rollout = 100\n",
    "batchsize = 500\n",
    "num_parallel_episodes = 16\n",
    "num_effective_episodes = 5001\n",
    "num_iter=10\n",
    "eval_num_parallel = 16\n",
    "gae_lambda = 0.999\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = Value(num_features=9, num_relations=len(relations_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_optimizer = torch.optim.Adam(policy.parameters(), lr=lr_policy)\n",
    "value_optimizer = torch.optim.Adam(value.parameters(), lr=lr_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = {\n",
    "    'policy_loss': [],\n",
    "    'kl_divergence': [],\n",
    "    'entropy': [],\n",
    "    'beta': [],\n",
    "    'value_loss': [],\n",
    "}\n",
    "progress = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 245/5001 [21:49:20<422:01:07, 319.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting episodes...\n",
      "Sampling episodes...\n",
      "Computing values...\n",
      "Computing returns...\n",
      "Computing advantages...\n",
      "Started training...\n",
      "Policy iteration 0 started...\n",
      "Policy iteration 1 started...\n",
      "Policy iteration 2 started...\n",
      "Policy iteration 3 started...\n",
      "Policy iteration 4 started...\n",
      "Policy iteration 5 started...\n",
      "Policy iteration 6 started...\n",
      "Policy iteration 7 started...\n",
      "Policy iteration 8 started...\n",
      "Policy iteration 9 started...\n",
      "Value iteration 0 started...\n",
      "Computing advantages...\n",
      "Computing returns...\n",
      "Computing value loss...\n",
      "Value iteration 1 started...\n",
      "Computing advantages...\n",
      "Computing returns...\n",
      "Computing value loss...\n",
      "Value iteration 2 started...\n",
      "Computing advantages...\n",
      "Computing returns...\n",
      "Computing value loss...\n",
      "Value iteration 3 started...\n",
      "Computing advantages...\n",
      "Computing returns...\n",
      "Computing value loss...\n",
      "Value iteration 4 started...\n",
      "Computing advantages...\n",
      "Computing returns...\n",
      "Computing value loss...\n",
      "Value iteration 5 started...\n",
      "Computing advantages...\n",
      "Computing returns...\n",
      "Computing value loss...\n",
      "Value iteration 6 started...\n",
      "Computing advantages...\n",
      "Computing returns...\n",
      "Computing value loss...\n",
      "Value iteration 7 started...\n",
      "Computing advantages...\n"
     ]
    }
   ],
   "source": [
    "for effective_episode_num in tqdm(range(num_effective_episodes)):\n",
    "\n",
    "    policy.eval()\n",
    "    policy.to('cpu')\n",
    "    print('Collecting episodes...')\n",
    "    _episodes = Parallel(n_jobs=-1, backend='multiprocessing')(\n",
    "        [delayed(get_episodes)(policy) for _ in range(num_parallel_episodes)]\n",
    "    )\n",
    "    episodes = []\n",
    "    for episode in _episodes:\n",
    "        episodes.extend(episode)\n",
    "    episodes = concat_episodes(episodes)\n",
    "\n",
    "    #with open('episodes_for_debugging.pickle', 'rb') as f:\n",
    "    #    episodes = pickle.load(f)\n",
    "    \n",
    "    print('Sampling episodes...')\n",
    "    sampled_episodes = [episodes.sample(rollout) for _ in range(batchsize)]\n",
    "    for sampled_episode in sampled_episodes:\n",
    "        sampled_episode['done'][-1] = True\n",
    "    sampled_episodes = concat_episodes(sampled_episodes)\n",
    "    \n",
    "    policy.train()\n",
    "    value.train()\n",
    "    \n",
    "    print('Computing values...')\n",
    "    compute_values(sampled_episodes, value, device=device)\n",
    "    print('Computing returns...')\n",
    "    compute_returns(sampled_episodes, gamma)\n",
    "    print('Computing advantages...')\n",
    "    compute_advantages(sampled_episodes, gamma, lambda_=gae_lambda, device=device)\n",
    "\n",
    "    policy_old = copy.deepcopy(policy)\n",
    "\n",
    "    print('Started training...')\n",
    "    policy.train()\n",
    "    for iter_num in range(num_iter):\n",
    "        print('Policy iteration {} started...'.format(iter_num))\n",
    "        ppo_losses = get_ppo_losses(\n",
    "            sampled_episodes,\n",
    "            policy,\n",
    "            policy_old,\n",
    "            clip=False,\n",
    "            compute_kl=True,\n",
    "            compute_entropy=True,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        policy_loss = ppo_losses['policy_loss']\n",
    "        kl_divergence = ppo_losses['kl']\n",
    "        entropy = ppo_losses['entropy']\n",
    "\n",
    "        loss = policy_loss + beta * kl_divergence - entropy_reg * entropy\n",
    "\n",
    "        if kl_divergence > 1.5 * target_kld:\n",
    "            print(\"Early stopping because of high KL-divergence.\")\n",
    "            break\n",
    "\n",
    "        policy.zero_grad()\n",
    "        loss.backward()\n",
    "        policy_optimizer.step()\n",
    "\n",
    "    if kl_divergence > 1.3 * target_kld:\n",
    "        beta *= 1.5\n",
    "    elif kl_divergence < 0.7 * target_kld:\n",
    "        beta /= 1.5\n",
    "\n",
    "    del sampled_episodes.episodes['value']\n",
    "    del sampled_episodes.episodes['return']\n",
    "    del sampled_episodes.episodes['advantage']\n",
    "    torch.cuda.empty_cache()\n",
    "        \n",
    "    for iter_num in range(num_iter):\n",
    "        print('Value iteration {} started...'.format(iter_num))\n",
    "        print('Computing advantages...')\n",
    "        compute_values(sampled_episodes, value, device=device, update=True)\n",
    "        print('Computing returns...')\n",
    "        compute_returns(sampled_episodes, gamma, update=True)\n",
    "        print('Computing value loss...')\n",
    "        value_loss = get_value_loss(sampled_episodes, device=device)\n",
    "        value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        value_optimizer.step()\n",
    "        \n",
    "    del sampled_episodes.episodes['value']\n",
    "    del sampled_episodes.episodes['return']\n",
    "    torch.cuda.empty_cache()\n",
    "        \n",
    "    policy_loss.to('cpu')\n",
    "    kl_divergence.to('cpu')\n",
    "    entropy.to('cpu')\n",
    "    value_loss.to('cpu')\n",
    "    log['policy_loss'].append(float(policy_loss.detach()))\n",
    "    log['kl_divergence'].append(float(kl_divergence.detach()))\n",
    "    log['entropy'].append(float(entropy.detach()))\n",
    "    log['beta'].append(beta)\n",
    "    log['value_loss'].append(float(value_loss.detach()))\n",
    "    \n",
    "    del ppo_losses, policy_loss, loss, kl_divergence, entropy, value_loss\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    policy.to('cpu')\n",
    "    policy.eval()\n",
    "    print('Training ended.')\n",
    "    #print(log)\n",
    "    with open('training_log.json', 'wt') as f:\n",
    "        json.dump(log, f)\n",
    "    clear_output()\n",
    "    if effective_episode_num % 10 == 0:\n",
    "        print('Evaluating...')\n",
    "        rewards_eval = Parallel(n_jobs=-1, backend='multiprocessing')(\n",
    "            [delayed(evaluate_agent)(policy) for _ in range(eval_num_parallel)]\n",
    "        )\n",
    "        progress.append(float(np.mean(rewards_eval)))\n",
    "        with open('reward_curve.json', 'wt') as f:\n",
    "            json.dump(progress, f)\n",
    "        plt.close()\n",
    "        plt.cla()\n",
    "        plt.clf()\n",
    "        clear_output()\n",
    "        plt.plot([i*10 for i in range(len(progress))], progress)\n",
    "        plt.grid()\n",
    "        plt.xlim(0, None)\n",
    "        plt.xlabel('number of effective episodes')\n",
    "        plt.ylabel('mean reward')\n",
    "        plt.show()\n",
    "    if effective_episode_num % 50 == 0:\n",
    "        torch.save(policy.to('cpu').state_dict(), './policies/{}.pth'.format(effective_episode_num))\n",
    "        torch.save(value.to('cpu').state_dict(), './values/{}.pth'.format(effective_episode_num))\n",
    "\n",
    "    del sampled_episodes\n",
    "    del episodes\n",
    "    del _episodes\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
