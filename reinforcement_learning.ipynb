{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial with Graph Convolutional Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 方針\n",
    "\n",
    "### 教師あり学習\n",
    "\n",
    "まずは教師あり学習により強いポリシーを獲得。\n",
    "\n",
    "### 自己対戦による訓練\n",
    "\n",
    "* Q関数が相手によって変わってしまう。\n",
    "    * On-policyで自己対戦にすればOK。Off-policyにしたい場合は、Q関数が大きく違わない過去エピソードとすべき。\n",
    "        * Rainbow\n",
    "            * PFRLに実装あり。まずはこれ？\n",
    "        * MuZero\n",
    "            * 実装少し大変かも。だが自己対戦による実績あるため性能は出るかも？\n",
    "    * Policyを持つアルゴリズムならOK。相手ごとにQ関数を学習すれば良い。この場合も自己対戦かつOn-policy。\n",
    "        * AC3、PPOなど\n",
    "            * PFRLに実装あり。これもトライ？\n",
    "            \n",
    "### 方策／Q関数モデル\n",
    "\n",
    "* Graph Neural Network\n",
    "    * あたかもそれぞれの選手が行動判断／価値判断しているようなモデルにする。選択されたアクションは、その時Activeな選手の最善手とする。\n",
    "    * 初めは、特徴量は、絶対位置座標で、完全グラフを用いる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions\n",
    "\n",
    "### Default action set\n",
    "\n",
    "The default action set consists of 19 actions:\n",
    "\n",
    "*   Idle actions\n",
    "\n",
    "    *   `action_idle` = 0, a no-op action, stickly actions are not affected (player maintains his directional movement etc.).\n",
    "\n",
    "*   Movement actions\n",
    "\n",
    "    *   `action_left` = 1, run to the left, sticky action.\n",
    "    *   `action_top_left` = 2, run to the top-left, sticky action.\n",
    "    *   `action_top` = 3, run to the top, sticky action.\n",
    "    *   `action_top_right` = 4, run to the top-right, sticky action.\n",
    "    *   `action_right` = 5, run to the right, sticky action.\n",
    "    *   `action_bottom_right` = 6, run to the bottom-right, sticky action.\n",
    "    *   `action_bottom` = 7, run to the bottom, sticky action.\n",
    "    *   `action_bottom_left` = 8, run to the bottom-left, sticky action.\n",
    "\n",
    "*   Passing / Shooting\n",
    "\n",
    "    *   `action_long_pass` = 9, perform a long pass to the player on your team. Player to pass the ball to is auto-determined based on the movement direction.\n",
    "    *   `action_high_pass` = 10, perform a high pass, similar to `action_long_pass`.\n",
    "    *   `action_short_pass` = 11, perform a short pass, similar to `action_long_pass`.\n",
    "    *   `action_shot` = 12, perform a shot, always in the direction of the opponent's goal.\n",
    "\n",
    "*   Other actions\n",
    "\n",
    "    *   `action_sprint` = 13, start sprinting, sticky action. Player moves faster, but has worse ball handling.\n",
    "    *   `action_release_direction` = 14, reset current movement direction.\n",
    "    *   `action_release_sprint` = 15, stop sprinting.\n",
    "    *   `action_sliding` = 16, perform a slide (effective when not having a ball).\n",
    "    *   `action_dribble` = 17, start dribbling (effective when having a ball), sticky action. Player moves slower, but it is harder to take over the ball from him.\n",
    "    *   `action_release_dribble` = 18, stop dribbling.\n",
    "\n",
    "### V2 action set\n",
    "\n",
    "It is an extension of the default action set:\n",
    "\n",
    "*   `action_builtin_ai` = 19, let game's built-in AI generate an action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-output": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'kaggle-environments' already exists and is not an empty directory.\n",
      "Processing /notebooks/kaggle/gfootball/kaggle-environments\n",
      "Requirement already satisfied: jsonschema>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kaggle-environments==1.3.14) (3.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kaggle-environments==1.3.14) (0.16.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kaggle-environments==1.3.14) (2.0.0)\n",
      "Requirement already satisfied: six>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kaggle-environments==1.3.14) (1.14.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kaggle-environments==1.3.14) (46.1.3.post20200325)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kaggle-environments==1.3.14) (19.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema>=3.0.1->kaggle-environments==1.3.14) (3.1.0)\n",
      "Building wheels for collected packages: kaggle-environments\n",
      "  Building wheel for kaggle-environments (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kaggle-environments: filename=kaggle_environments-1.3.14-py3-none-any.whl size=302295 sha256=4ff1eab431d2a148af1b404dce2ce65c241d7b6dbb393a1d65fb8d067435f101\n",
      "  Stored in directory: /root/.cache/pip/wheels/32/ff/b6/a9ab62cd5f60b2492aa5d5bc96a6d12bb1158496e87a4576ec\n",
      "Successfully built kaggle-environments\n",
      "Installing collected packages: kaggle-environments\n",
      "  Attempting uninstall: kaggle-environments\n",
      "    Found existing installation: kaggle-environments 1.3.14\n",
      "    Uninstalling kaggle-environments-1.3.14:\n",
      "      Successfully uninstalled kaggle-environments-1.3.14\n",
      "Successfully installed kaggle-environments-1.3.14\n",
      "Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
      "Ign:2 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
      "Hit:3 http://archive.ubuntu.com/ubuntu bionic InRelease                       \n",
      "Ign:4 http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
      "Hit:5 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
      "Hit:6 http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
      "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]      \n",
      "Hit:10 http://packages.cloud.google.com/apt gcsfuse-bionic InRelease           \n",
      "Hit:11 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease         \n",
      "Hit:12 http://packages.cloud.google.com/apt cloud-sdk InRelease                \n",
      "Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]   \n",
      "Fetched 252 kB in 2s (128 kB/s)    \n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "libsdl2-gfx-dev is already the newest version (1.0.4+dfsg-1).\n",
      "libsdl2-ttf-dev is already the newest version (2.0.14+dfsg1-2).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 106 not upgraded.\n",
      "fatal: destination path 'football' already exists and is not an empty directory.\n",
      "--2020-11-21 10:39:20--  https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.3.so\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.175.48, 172.217.175.80, 172.217.175.112, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.175.48|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 45163384 (43M) [application/octet-stream]\n",
      "Saving to: ‘football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so’\n",
      "\n",
      "football/third_part 100%[===================>]  43.07M  11.0MB/s    in 4.0s    \n",
      "\n",
      "2020-11-21 10:39:25 (10.9 MB/s) - ‘football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so’ saved [45163384/45163384]\n",
      "\n",
      "Processing /notebooks/kaggle/gfootball/football\n",
      "Requirement already satisfied: pygame==1.9.6 in /opt/conda/lib/python3.7/site-packages (from gfootball==2.3) (1.9.6)\n",
      "Requirement already satisfied: opencv-python in /opt/conda/lib/python3.7/site-packages (from gfootball==2.3) (4.4.0.44)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from gfootball==2.3) (1.4.1)\n",
      "Requirement already satisfied: gym>=0.11.0 in /opt/conda/lib/python3.7/site-packages (from gfootball==2.3) (0.17.3)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.7/site-packages (from gfootball==2.3) (0.10.0)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from gfootball==2.3) (0.34.2)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /opt/conda/lib/python3.7/site-packages (from opencv-python->gfootball==2.3) (1.18.5)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from gym>=0.11.0->gfootball==2.3) (1.3.0)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from gym>=0.11.0->gfootball==2.3) (1.5.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from absl-py->gfootball==2.3) (1.14.0)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.11.0->gfootball==2.3) (0.18.2)\n",
      "Building wheels for collected packages: gfootball\n",
      "  Building wheel for gfootball (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gfootball: filename=gfootball-2.3-cp37-cp37m-linux_x86_64.whl size=38649989 sha256=41302900a3a11febf369d44ee281a69a55bf048b3feb93d165cdb43d2b7ffe00\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-7yz0alxx/wheels/2e/fa/d3/e08f039bbd2ffb7dc6fdc617e25eb16978d5b56f9e2f5638dc\n",
      "Successfully built gfootball\n",
      "Installing collected packages: gfootball\n",
      "  Attempting uninstall: gfootball\n",
      "    Found existing installation: gfootball 2.3\n",
      "    Uninstalling gfootball-2.3:\n",
      "      Successfully uninstalled gfootball-2.3\n",
      "Successfully installed gfootball-2.3\n"
     ]
    }
   ],
   "source": [
    "# Install:\n",
    "# Kaggle environments.\n",
    "!git clone https://github.com/Kaggle/kaggle-environments.git\n",
    "!cd kaggle-environments && pip install .\n",
    "\n",
    "# GFootball environment.\n",
    "!apt-get update -y\n",
    "!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev\n",
    "\n",
    "# Make sure that the Branch in git clone and in wget call matches !!\n",
    "!git clone -b v2.3 https://github.com/google-research/football.git\n",
    "!mkdir -p football/third_party/gfootball_engine/lib\n",
    "\n",
    "!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.3.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so\n",
    "!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pfrl==0.1.0 in /opt/conda/lib/python3.7/site-packages (0.1.0)\n",
      "Requirement already satisfied: gym>=0.9.7 in /opt/conda/lib/python3.7/site-packages (from pfrl==0.1.0) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /opt/conda/lib/python3.7/site-packages (from pfrl==0.1.0) (1.18.5)\n",
      "Requirement already satisfied: torch>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from pfrl==0.1.0) (1.6.0)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (from pfrl==0.1.0) (8.0.0)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from gym>=0.9.7->pfrl==0.1.0) (1.3.0)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from gym>=0.9.7->pfrl==0.1.0) (1.5.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from gym>=0.9.7->pfrl==0.1.0) (1.4.1)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch>=1.3.0->pfrl==0.1.0) (0.18.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pfrl==0.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ install torch_geometric begin -----------------\n",
    "try:\n",
    "    import torch_geometric\n",
    "except:\n",
    "    import subprocess\n",
    "    import torch\n",
    "\n",
    "    nvcc_stdout = str(subprocess.check_output(['nvcc', '-V']))\n",
    "    tmp = nvcc_stdout[nvcc_stdout.rfind('release') + len('release') + 1:]\n",
    "    cuda_version = tmp[:tmp.find(',')]\n",
    "    cuda = {\n",
    "            '9.2': 'cu92',\n",
    "            '10.1': 'cu101',\n",
    "            '10.2': 'cu102',\n",
    "            }\n",
    "\n",
    "    CUDA = cuda[cuda_version]\n",
    "    TORCH = torch.__version__.split('.')\n",
    "    TORCH[-1] = '0'\n",
    "    TORCH = '.'.join(TORCH)\n",
    "\n",
    "    install1 = 'pip install torch-scatter==latest+' + CUDA + ' -f https://pytorch-geometric.com/whl/torch-' + TORCH + '.html'\n",
    "    install2 = 'pip install torch-sparse==latest+' + CUDA + ' -f https://pytorch-geometric.com/whl/torch-' + TORCH + '.html'\n",
    "    install3 = 'pip install torch-cluster==latest+' + CUDA + ' -f https://pytorch-geometric.com/whl/torch-' + TORCH + '.html'\n",
    "    install4 = 'pip install torch-spline-conv==latest+' + CUDA + ' -f https://pytorch-geometric.com/whl/torch-' + TORCH + '.html'\n",
    "    install5 = 'pip install torch-geometric'\n",
    "\n",
    "    subprocess.run(install1.split())\n",
    "    subprocess.run(install2.split())\n",
    "    subprocess.run(install3.split())\n",
    "    subprocess.run(install4.split())\n",
    "    subprocess.run(install5.split())\n",
    "# ------------------ install torch_geometric end -----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import glob \n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "import copy\n",
    "import imageio\n",
    "import pathlib\n",
    "import collections\n",
    "from collections import deque, namedtuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from joblib import Parallel, delayed\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "\n",
    "from gym import spaces\n",
    "from tqdm import tqdm\n",
    "from logging import getLogger, StreamHandler, FileHandler, DEBUG, INFO\n",
    "from typing import Union, Callable, List, Tuple, Iterable, Any, Dict\n",
    "from dataclasses import dataclass\n",
    "from IPython.display import Image, display\n",
    "sns.set()\n",
    "\n",
    "\n",
    "# PFRL\n",
    "import pfrl\n",
    "from pfrl.agents import CategoricalDoubleDQN\n",
    "from pfrl import experiments\n",
    "from pfrl import explorers\n",
    "from pfrl import nn as pnn\n",
    "from pfrl import utils\n",
    "from pfrl import replay_buffers\n",
    "from pfrl.wrappers import atari_wrappers\n",
    "from pfrl.q_functions import DistributionalDuelingDQN\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as D\n",
    "\n",
    "# PyTorch geometric\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import RGCNConv\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "\n",
    "from torch_scatter import scatter_max, scatter_sum, scatter_mean\n",
    "\n",
    "# Env\n",
    "import gym\n",
    "import gfootball\n",
    "import gfootball.env as football_env\n",
    "from gfootball.env import observation_preprocessing\n",
    "from gfootball.env.wrappers import Simple115StateWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Check we can use GPU\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# set gpu id\n",
    "if torch.cuda.is_available(): \n",
    "    # NOTE: it is not number of gpu but id which start from 0\n",
    "    gpu = 0\n",
    "else:\n",
    "    # cpu=>-1\n",
    "    gpu = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set logger\n",
    "def logger_config():\n",
    "    logger = getLogger(__name__)\n",
    "    handler = StreamHandler()\n",
    "    handler.setLevel(\"DEBUG\")\n",
    "    logger.setLevel(\"DEBUG\")\n",
    "    logger.addHandler(handler)\n",
    "    logger.propagate = False\n",
    "\n",
    "    filepath = './result.log'\n",
    "    file_handler = FileHandler(filepath)\n",
    "    logger.addHandler(file_handler)\n",
    "    return logger\n",
    "\n",
    "logger = logger_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed random seed\n",
    "# but this is NOT enough to fix the result of rewards.Please tell me the reason.\n",
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    utils.set_random_seed(seed)  # for PFRL\n",
    "    \n",
    "# Set a random seed used in PFRL.\n",
    "seed = 5046\n",
    "seed_everything(seed)\n",
    "\n",
    "# Set different random seeds for train and test envs.\n",
    "train_seed = seed\n",
    "test_seed = 2 ** 31 - 1 - seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = football_env.create_environment(\n",
    "    env_name='11_vs_11_kaggle',  # easy mode\n",
    "    stacked=False,\n",
    "    representation='simple115v2',           # SMM\n",
    "    rewards='scoring, checkpoints',\n",
    "    write_goal_dumps=False,\n",
    "    write_full_episode_dumps=False,\n",
    "    render=False,\n",
    "    write_video=False,\n",
    "    dump_frequency=1,\n",
    "    logdir='./',\n",
    "    extra_players=None,\n",
    "    number_of_left_players_agent_controls=1,\n",
    "    number_of_right_players_agent_controls=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, _, _, info = env.step([1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCN-Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The flag to distinguish left players, right players, and the ball.\n",
    "\n",
    "* left players: 0\n",
    "* right players: 1\n",
    "* ball: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_dict = {\n",
    "    (0., 0.): 0, # left player -> left player\n",
    "    (0., 1.): 1, # left player -> right player\n",
    "    (0., 2.): 2, # left player -> ball\n",
    "    (1., 0.): 3, # right player -> left player\n",
    "    (1., 1.): 4, # right player -> right player\n",
    "    (1., 2.): 5, # right player -> ball\n",
    "    (2., 0.): 6, # ball -> left player\n",
    "    (2., 1.): 7, # ball -> right player\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_from_observation(state, action=None, reverse=False, reverse_y=False):\n",
    "    array = state\n",
    "    \n",
    "    left_coordinations = np.concatenate([array[:22].reshape(11, 2), np.zeros((11, 1))], axis=-1)\n",
    "    left_directions = np.concatenate([array[22:44].reshape(11, 2), np.zeros((11, 1))], axis=-1)\n",
    "    right_coordinations = np.concatenate([array[44:66].reshape(11, 2), np.zeros((11, 1))], axis=-1)\n",
    "    right_directions = np.concatenate([array[66:88].reshape(11, 2), np.zeros((11, 1))], axis=-1)\n",
    "    ball_coordination = array[88:91].reshape([1, 3])\n",
    "    ball_direction = array[91:94].reshape([1, 3])\n",
    "    ball_ownership = array[94:97] # none, left, right\n",
    "    active_player = array[97:108].reshape([11, 1])\n",
    "    game_mode = array[108:]\n",
    "    \n",
    "    if reverse:\n",
    "        left_coordinations[:, 0] *= -1.0\n",
    "        left_directions[:, 0] *= -1.0\n",
    "        right_coordinations[:, 0] *= -1.0\n",
    "        right_directions[:, 0] *= -1.0\n",
    "        ball_coordination[:, 0] *= -1.0\n",
    "        ball_direction[:, 0] *= -1.0\n",
    "        ball_ownership = ball_ownership[[0, 2, 1]]\n",
    "        active_player = active_player\n",
    "        game_mode = game_mode\n",
    "\n",
    "    if reverse_y:\n",
    "        left_coordinations[:, 1] *= -1.0\n",
    "        left_directions[:, 1] *= -1.0\n",
    "        right_coordinations[:, 1] *= -1.0\n",
    "        right_directions[:, 1] *= -1.0\n",
    "        ball_coordination[:, 1] *= -1.0\n",
    "        ball_direction[:, 1] *= -1.0\n",
    "        ball_ownership = ball_ownership\n",
    "        active_player = active_player\n",
    "        game_mode = game_mode\n",
    "\n",
    "    # Node features\n",
    "    left_features = np.concatenate([\n",
    "        0*np.ones((11, 1)),\n",
    "        left_coordinations,\n",
    "        left_directions,\n",
    "        ball_ownership[1]*np.ones((11, 1)),\n",
    "        active_player,\n",
    "    ], axis=-1)\n",
    "    right_features = np.concatenate([\n",
    "        1*np.ones((11, 1)),\n",
    "        right_coordinations,\n",
    "        right_directions,\n",
    "        ball_ownership[2]*np.ones((11, 1)),\n",
    "        np.zeros((11, 1)),\n",
    "    ], axis=-1)\n",
    "    ball_features = np.concatenate([\n",
    "        2*np.ones((1, 1)),\n",
    "        ball_coordination,\n",
    "        ball_direction,\n",
    "        np.zeros((1, 1)),\n",
    "        np.zeros((1, 1)),\n",
    "    ], axis=-1)\n",
    "\n",
    "    features = np.concatenate([left_features, right_features, ball_features], axis=0)\n",
    "\n",
    "    # Edges and relations\n",
    "    X, Y = np.meshgrid(np.arange(len(features)), np.arange(len(features)))\n",
    "    all_combinations = np.vstack([X.flatten(), Y.flatten()]).T\n",
    "    edge_index = np.array(\n",
    "        [combination for combination in all_combinations if not combination[0] == combination[1]]\n",
    "    ).T\n",
    "    types_for_edge_index = features[edge_index][:,:,0]\n",
    "    relations = [relations_dict[tuple(types)] for types in types_for_edge_index.T]\n",
    "\n",
    "    # numpy array to torch tensor\n",
    "    features = torch.tensor(features, dtype=torch.float).contiguous()\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).contiguous()\n",
    "    relations = torch.tensor(relations, dtype=torch.long).contiguous()\n",
    "\n",
    "    if action is None:\n",
    "        graph = Data(x=features, edge_index=edge_index, edge_type=relations)\n",
    "\n",
    "    else:\n",
    "        graph = Data(x=features, edge_index=edge_index, edge_type=relations, y=torch.tensor(action, dtype=torch.long))\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_relations):\n",
    "        super().__init__()\n",
    "        self.conv1 = RGCNConv(num_features - 2, 256, num_relations=num_relations)\n",
    "        #self.conv2 = RGCNConv(128, 256, num_relations=num_relations)\n",
    "        self.fc1 = nn.Linear(256, 256)\n",
    "        self.fc2 = nn.Linear(256, 19)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_type = data.x, data.edge_index, data.edge_type\n",
    "        flag = x[:,0]\n",
    "        is_active = x[:,-1]\n",
    "        x = x[:,1:-1]\n",
    "\n",
    "        x = self.conv1(x, edge_index, edge_type)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        #x = self.conv2(x, edge_index, edge_type)\n",
    "        #x = F.relu(x)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return D.Categorical(logits=x[is_active.bool()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_relations):\n",
    "        super().__init__()\n",
    "        self.conv1 = RGCNConv(num_features - 2, 256, num_relations=num_relations)\n",
    "        #self.conv2 = RGCNConv(128, 256, num_relations=num_relations)\n",
    "        self.fc1 = nn.Linear(256, 256)\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_type = data.x, data.edge_index, data.edge_type\n",
    "        flag = x[:,0]\n",
    "        is_active = x[:,-1]\n",
    "        x = x[:,1:-1]\n",
    "\n",
    "        x = self.conv1(x, edge_index, edge_type)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        #x = self.conv2(x, edge_index, edge_type)\n",
    "        #x = F.relu(x)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        if hasattr(data, 'batch'):\n",
    "            x, argmax = scatter_max(x, data.batch, dim=0)\n",
    "        else:\n",
    "            x = torch.max(x, dim=0).values\n",
    "\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load initial policy (Supervised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy(num_features=9, num_relations=len(relations_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_path = 'policy.pt'\n",
    "policy.load_state_dict(torch.load(policy_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Episodes:\n",
    "    def __init__(\n",
    "        self,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.episodes = {\n",
    "            **kwargs,\n",
    "        }\n",
    "        for key in ('state', 'action', 'next_state', 'reward', 'done'):\n",
    "            if key not in kwargs.keys():\n",
    "                self.episodes[key] = []\n",
    "        for key, value in self.episodes.items():\n",
    "            if len(value) != len(self.episodes['state']):\n",
    "                raise Exception('The length of {} is invalid.'.format(key))\n",
    "        \n",
    "    def append(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            self.episodes[key].append(value)\n",
    "        \n",
    "    def add_new_key(self, key, value):\n",
    "        if len(value) != self._length:\n",
    "            raise Exception('The length of {} is invalid.'.format(key))\n",
    "        elif key in self.episodes.keys():\n",
    "            raise Exception('The key {} is already defined.'.format(key))\n",
    "        self.episodes[key] = value\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.episodes['state'])\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'Episodes\\n* number of episodes: {}\\n* number of steps: {}\\n* total rewards: {}\\n* keys: {}'.format(\n",
    "            np.sum(self.episodes['done']),\n",
    "            len(self),\n",
    "            np.sum(self.episodes['reward']),\n",
    "            ', '.join(self.episodes.keys()),\n",
    "        )\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        return self.episodes[key]\n",
    "\n",
    "    def sample(self, size, sequential=True):\n",
    "        if sequential:\n",
    "            id_from = random.randint(0, len(self) - size)\n",
    "            id_to = id_from + size\n",
    "            sampled_dict = {\n",
    "                key: value[id_from:id_to]\n",
    "                for key, value\n",
    "                in self.episodes.items()\n",
    "            }\n",
    "            sampled_episode = Episodes(\n",
    "                **sampled_dict\n",
    "            )\n",
    "            return sampled_episode\n",
    "        else:\n",
    "            raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Episodic functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_returns(episodes, gamma, value_net, infinite_horizon=True):\n",
    "    returns = copy.deepcopy(episodes.episodes['reward'])\n",
    "    states = episodes.episodes['state']\n",
    "    dones = episodes.episodes['done']\n",
    "    for t in reversed(range(len(rews))):\n",
    "        returns[t] = last_reward_to_go = (\n",
    "            returns[t] \n",
    "            + (1 - dones[t]) * gamma * last_reward_to_go\n",
    "        )\n",
    "        if inifinite_horizon and dones[t]:\n",
    "            returns[t] += value_net(states[t])\n",
    "    episodes.add_new_key('return', returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_values(episodes, value_net):\n",
    "    values = [value_net(state) for state in episodes.episodes['state']]\n",
    "    episodes.add_new_key('value', values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_episodes(episodes_list):\n",
    "    keys = episodes_list[0].episodes.keys()\n",
    "    episodes_dict = {key: [] for key in keys}\n",
    "    for episodes in episodes_list:\n",
    "        for key in keys:\n",
    "            episodes_dict[key] += episodes.episodes[key]\n",
    "    return Episodes(**episodes_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "REVERSE_ACTION = {i: i for i in range(20)}\n",
    "REVERSE_ACTION[1] = 5\n",
    "REVERSE_ACTION[5] = 1\n",
    "REVERSE_ACTION[2] = 4\n",
    "REVERSE_ACTION[4] = 2\n",
    "REVERSE_ACTION[6] = 8\n",
    "REVERSE_ACTION[8] = 6\n",
    "def reverse_action(action):\n",
    "    return REVERSE_ACTION[action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = football_env.create_environment(\n",
    "    env_name='11_vs_11_kaggle',  # easy mode\n",
    "    stacked=False,\n",
    "    representation='simple115v2',\n",
    "    rewards='scoring, checkpoints',\n",
    "    write_goal_dumps=False,\n",
    "    write_full_episode_dumps=False,\n",
    "    render=False,\n",
    "    write_video=False,\n",
    "    dump_frequency=1,\n",
    "    logdir='./',\n",
    "    extra_players=None,\n",
    "    number_of_left_players_agent_controls=1,\n",
    "    number_of_right_players_agent_controls=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_episodes(policy):\n",
    "    \n",
    "    episode_l = Episodes()\n",
    "    episode_r = Episodes()\n",
    "\n",
    "    obs_l, obs_r = env.reset()\n",
    "    next_state_l = create_graph_from_observation(obs_l)\n",
    "    next_state_r = create_graph_from_observation(obs_r)\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        state_l, state_r = next_state_l, next_state_r\n",
    "        states = torch_geometric.data.Batch.from_data_list([\n",
    "            state_l, state_r\n",
    "        ])\n",
    "\n",
    "        action_l, action_r = policy(states).sample().detach().numpy().tolist()\n",
    "        #action_r = reverse_action(action_r_rev)\n",
    "\n",
    "        (obs_l, obs_r), (reward_l, reward_r), done, info = env.step([action_l, action_r])\n",
    "\n",
    "        next_state_l = create_graph_from_observation(obs_l)\n",
    "        next_state_r = create_graph_from_observation(obs_r)\n",
    "\n",
    "        episode_l.append(\n",
    "            state=state_l,\n",
    "            action=action_l,\n",
    "            reward=reward_l,\n",
    "            next_state=next_state_l,\n",
    "            done=done,\n",
    "        )\n",
    "        episode_r.append(\n",
    "            state=state_r,\n",
    "            action=action_r,\n",
    "            reward=reward_r,\n",
    "            next_state=next_state_r,\n",
    "            done=done,\n",
    "        )\n",
    "        \n",
    "    return episode_l, episode_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min, sys: 56.2 ms, total: 1min\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "episodes = get_episodes(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend MultiprocessingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  10 | elapsed:  1.3min remaining:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:  2.4min remaining:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:  2.4min remaining:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:  2.4min remaining:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  10 | elapsed:  2.4min remaining:   36.7s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  3.5min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  3.5min finished\n"
     ]
    }
   ],
   "source": [
    "policy.eval()\n",
    "episodes = Parallel(n_jobs=-1, backend='multiprocessing', verbose=100)([delayed(get_episodes)(policy) for _ in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(obs):\n",
    "    global policy\n",
    "    \n",
    "    # Get observations for the first (and only one) player we control.\n",
    "    #obs = Simple115StateWrapper.convert_observation(obs['players_raw'], True)\n",
    "    graph = create_graph_from_observation(obs)\n",
    "    actions = policy(graph)\n",
    "    action = int(np.argmax(actions.detach().numpy()))\n",
    "    return [action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(agent, env, n_times=50):\n",
    "    scores = []\n",
    "    for _ in tqdm(range(50)):\n",
    "        obs = env.reset()\n",
    "        rewards = []\n",
    "        while True:\n",
    "            action = agent([obs])\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "        score = np.sum(rewards)\n",
    "        print(score)\n",
    "        scores.append(score)\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "_episodes = episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = []\n",
    "for episode in _episodes:\n",
    "    episodes.extend(episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "concated_episode = concat_episodes(episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_episode = concated_episode.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "calculate_returns() missing 2 required positional arguments: 'gamma' and 'value_net'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-edefc2635cce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcalculate_returns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_episode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: calculate_returns() missing 2 required positional arguments: 'gamma' and 'value_net'"
     ]
    }
   ],
   "source": [
    "calculate_returns(sampled_episode, gamma=0.997,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = Value(num_features=9, num_relations=len(relations_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = torch_geometric.data.Batch.from_data_list(sampled_episode['state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0078],\n",
       "        [0.0058],\n",
       "        [0.0017],\n",
       "        [0.0039],\n",
       "        [0.0027],\n",
       "        [0.0046],\n",
       "        [0.0066],\n",
       "        [0.0068],\n",
       "        [0.0060],\n",
       "        [0.0024]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_returns()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
